{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained object detection model\n",
    "net =  cv2.dnn.readNetFromDarknet( 'yolov3.cfg','yolov3.weights')\n",
    "\n",
    "# Define the threshold for object detection\n",
    "threshold = 0.6\n",
    "\n",
    "# Define the video URL\n",
    "url = \"https://www.youtube.com/watch?v=AFEZzf9_EHk\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "# Initialize a counter for the extracted clips\n",
    "clip_count = 0\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the video stream has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame for object detection\n",
    "    resized = cv2.resize(frame, (300, 300))\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    blob = cv2.dnn.blobFromImage(resized, 0.007843, (300, 300), (127.5, 127.5, 127.5), False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Loop over the detections\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # Check if the confidence score is greater than the threshold\n",
    "        if confidence > threshold:\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "            # Check if the detected object is a cricket bat\n",
    "            if class_id == 77:\n",
    "                # Extract the coordinates of the bounding box\n",
    "                box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # Extract the clip of the batsman before and after hitting the ball\n",
    "                clip_length = 6  # Length of the clip in seconds\n",
    "                clip_start = max(0, int(startX - (clip_length/2) * (endX - startX)))\n",
    "                clip_end = min(frame.shape[1], int(endX + (clip_length/2) * (endX - startX)))\n",
    "                clip = frame[:, clip_start:clip_end]\n",
    "\n",
    "                # Save the clip to a file\n",
    "                filename = \"clip\" + str(clip_count) + \".mp4\"\n",
    "                cv2.imwrite(filename, clip)\n",
    "\n",
    "                # Increment the clip counter\n",
    "                clip_count += 1\n",
    "\n",
    "# Release the video stream\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv2.dnn.readNetFromDarknet( 'C:/Users/Yash/models/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/yolov3.cfg','C:/Users/Yash/models/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/yolov3.weights')\n",
    "model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bat(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    model.setInput(blob)\n",
    "    outputs = model.forward(model.getUnconnectedOutLayersNames())\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = center_x - w//2\n",
    "                y = center_y - h//2\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        x, y, w, h = boxes[i]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from pytube import YouTube\n",
    "\n",
    "def extract_clips(video_link):\n",
    "    # Download the YouTube video and get the video stream\n",
    "    yt = YouTube(video_link)\n",
    "    stream = yt.streams.filter(progressive=True).first()\n",
    "\n",
    "    # Open the video stream with OpenCV\n",
    "    cap = cv2.VideoCapture(stream.url)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "    clip_duration = 2  # duration of each clip in seconds\n",
    "    clips = []\n",
    "    \n",
    "    # Iterate over the video frames and extract clips\n",
    "    for i in range(math.ceil(duration / clip_duration)):\n",
    "        start_time = i * clip_duration\n",
    "        end_time = min((i + 1) * clip_duration, duration)\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "        clip = []\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        # Iterate over the frames in the clip\n",
    "        while True:\n",
    "            # Read the frame from the video stream\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) >= end_frame:\n",
    "                break\n",
    "            \n",
    "            # Perform video analysis to detect batsmen hitting the ball\n",
    "            # Here's an example implementation using OpenCV's HOGDescriptor\n",
    "            hog = cv2.HOGDescriptor()\n",
    "            hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "            rects, _ = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "            if len(rects) > 0:\n",
    "                # A ball was detected in the current frame, add it to the clip\n",
    "                clip.append(frame)\n",
    "                \n",
    "        # Add the clip to the list of clips if it contains frames\n",
    "        if clip:\n",
    "            clips.append(np.stack(clip))\n",
    "    \n",
    "    # Release the video stream\n",
    "    cap.release()\n",
    "    \n",
    "    return clips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=Fj_WWSAJ2qw\"\n",
    "extract_clips(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "total time 541.6340599060059\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from pytube import YouTube\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "tic=time.time()\n",
    "\n",
    "# Set the video URL\n",
    "url1 = \"https://www.youtube.com/watch?v=Fj_WWSAJ2qw\"\n",
    "url=\"https://www.youtube.com/watch?v=AFEZzf9_EHk\"\n",
    "\n",
    "# Set the output directory for the clips\n",
    "output_dir = \"bat_clips\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set the parameters for bat detection\n",
    "bat_confidence_threshold = 0.5\n",
    "clip_duration = 5  # duration of each clip in seconds\n",
    "bat_detection_window_start = -2  # start time of the detection window in seconds\n",
    "bat_detection_window_end = 3  # end time of the detection window in seconds\n",
    "\n",
    "# Download the YouTube video and get the video stream\n",
    "yt = YouTube(url)\n",
    "stream = yt.streams.filter(progressive=True).first()\n",
    "\n",
    "# Open the video stream with OpenCV\n",
    "cap = cv2.VideoCapture(stream.url)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = frame_count / fps\n",
    "\n",
    "# Load the bat detection model\n",
    "model = cv2.dnn.readNetFromDarknet('yolov3.cfg',\n",
    "                                   'yolov3.weights')\n",
    "model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Initialize the list of start and end times\n",
    "start_times = []\n",
    "end_times = []\n",
    "\n",
    "# Iterate over the video frames and extract clips\n",
    "for i in range(frame_count):\n",
    "    # Read the frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform bat detection\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), swapRB=True, crop=False)\n",
    "    model.setInput(blob)\n",
    "    outputs = model.forward(model.getUnconnectedOutLayersNames())\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > bat_confidence_threshold:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = center_x - w // 2\n",
    "                y = center_y - h // 2\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # If a bat is detected, add the start and end times to the list\n",
    "    if boxes:\n",
    "        start_time = max(0, i / fps + bat_detection_window_start)\n",
    "        end_time = min(duration, i / fps + bat_detection_window_end)\n",
    "        start_times.append(start_time)\n",
    "        end_times.append(end_time)\n",
    "\n",
    "# Release the video stream\n",
    "cap.release()\n",
    "\n",
    "print(\"total time\",time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9494423158790974, 7.739225202213708, 14.285261813537677, 21.948914431673053, 48.4523797360579, 52.60352490421456, 57.552967220093656, 66.01491698595147, 78.46835249042145, 97.94680289484887, 101.61896977437208, 123.97128991060026, 128.4417539378459, 135.7860876968923]\n",
      "[7.949442315879097, 12.739225202213708, 19.285261813537677, 26.948914431673053, 53.4523797360579, 57.60352490421456, 62.552967220093656, 71.01491698595147, 83.46835249042145, 102.94680289484887, 106.61896977437208, 128.97128991060026, 133.4417539378459, 140.7860876968923]\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[]\n",
    "e=[]\n",
    "for i in range(1,len(start_times)):\n",
    "    if(start_times[i]-start_times[i-1]>2):\n",
    "        s.append(start_times[i])\n",
    "    if(start_times[i]-start_times[i-1]>2):\n",
    "        e.append(end_times[i])\n",
    "print(s)\n",
    "print(e)\n",
    "start_times=s\n",
    "end_times=e\n",
    "print(len(end_times))\n",
    "len(start_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "Saved 14 clips to bat_clips\n",
      "total time 256.99871158599854\n"
     ]
    }
   ],
   "source": [
    "tic=time.time()\n",
    "print(\"started\")\n",
    "\n",
    "for i in range(len(start_times)):\n",
    "    start_time = start_times[i]\n",
    "    end_time = end_times[i]\n",
    "    clip_start_frame = int(start_time * fps)\n",
    "    clip_end_frame = int(end_time * fps)\n",
    "    clip_duration_frames = clip_end_frame - clip_start_frame\n",
    "    clip_frames = []\n",
    "    \n",
    "    for j in range(clip_duration_frames):\n",
    "        frame_idx = clip_start_frame + j\n",
    "        cap = cv2.VideoCapture(stream.url)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        clip_frames.append(frame)\n",
    "        \n",
    "    # Save the clip frames as a video file\n",
    "    clip_name = f\"bat_clip_{i+1}.mp4\"\n",
    "    clip_path = os.path.join(output_dir, clip_name)\n",
    "    out = cv2.VideoWriter(clip_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    for frame in clip_frames:\n",
    "        out.write(frame)\n",
    "        \n",
    "    out.release()\n",
    "    \n",
    "print(f\"Saved {len(start_times)} clips to {output_dir}\")\n",
    "print(\"total time\",time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-eddc28183bf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbowler_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;31m# Pass the cricket ball bounding box through the cricket ball detection model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mball_blob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m608\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m608\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mball_blob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print('hi')\n",
    "# Load YOLOv3 object detection model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Load the cricket ball detection model\n",
    "model = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Initialize the list of start and end times\n",
    "start_times = []\n",
    "\n",
    "# Set the confidence threshold and non-maximum suppression threshold\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# Initialize an empty list to store the timestamps\n",
    "timestamps = []\n",
    "\n",
    "# Initialize a flag to indicate if the bowler is about to bowl\n",
    "bowler_ready = False\n",
    "\n",
    "# Initialize a counter to keep track of frames saved after the bowler is ready\n",
    "frames_saved = 0\n",
    "url=\"https://www.youtube.com/watch?v=s8OJgjKlxnk\"\n",
    "yt = YouTube(url)\n",
    "stream = yt.streams.filter(progressive=True).first()\n",
    "\n",
    "# Open the video stream with OpenCV\n",
    "cap = cv2.VideoCapture(stream.url)\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was successfully read\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the dimensions of the frame\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Create a blob from the frame and pass it through the YOLOv3 model\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Initialize empty lists to store the detected objects' bounding boxes, confidences, and class IDs\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Loop over each detection in the output of the YOLOv3 model\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = center_x - w // 2\n",
    "                y = center_y - h // 2\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to remove redundant detections\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "    # Loop over each detection that survived non-maximum suppression\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Check if the detected object is a cricket ball\n",
    "        if class_ids[i] == 0:\n",
    "            # Draw a bounding box around the cricket ball\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Check if the bowler is ready to bowl\n",
    "            if not bowler_ready:\n",
    "                # Pass the cricket ball bounding box through the cricket ball detection model\n",
    "                ball_blob = cv2.dnn.blobFromImage(frame[y:y+h, x:x+w], 1/255, (608, 608), (0, 0, 0), swapRB=True, crop=False)\n",
    "                model.setInput(ball_blob)\n",
    "\n",
    "                # Perform forward pass to get output\n",
    "                ball_output = model.forward(model.getUnconnectedOutLayersNames()[0])\n",
    "\n",
    "                # Get class ID of the detected object\n",
    "                class_id = np.argmax(ball_output[0])\n",
    "\n",
    "                # Check if the detected object is a bowler\n",
    "                if class_id == 0:\n",
    "                    # Set the bowler_ready flag to True\n",
    "                    bowler_ready = True\n",
    "\n",
    "                    # Add the start time to the list of start times\n",
    "                    start_times.append(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "            \n",
    "            else:\n",
    "                # If the bowler is ready, save the current frame\n",
    "                cv2.imwrite(f\"frame{frames_saved}.jpg\", frame)\n",
    "\n",
    "                # Increment the counter for the number of frames saved after the bowler is ready\n",
    "                frames_saved += 1\n",
    "\n",
    "                # If 10 frames have been saved, stop saving frames and reset the bowler_ready flag and frames_saved counter\n",
    "                if frames_saved == 10:\n",
    "                    bowler_ready = False\n",
    "                    frames_saved = 0\n",
    "\n",
    "                    # Add the end time to the list of timestamps\n",
    "                    timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "    # Show the frame with the detected objects\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video stream and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the start and end times of the detected bowler ready poses\n",
    "for i in range(len(start_times)):\n",
    "    print(f\"Start time: {start_times[i]}, End time: {timestamps[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 2764800 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d97a4522b6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mvideo_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.youtube.com/watch?v=Fj_WWSAJ2qw\"\u001b[0m  \u001b[1;31m# Replace with the URL of the video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0msave_batsmen_clips\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8d97a4522b6d>\u001b[0m in \u001b[0;36mextract_frames\u001b[1;34m(video_url)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 2764800 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytube\n",
    "\n",
    "# Function to extract frames from the video\n",
    "def extract_frames(video_url):\n",
    "    video = pytube.YouTube(video_url)\n",
    "    stream = video.streams.get_by_itag(22)\n",
    "    file = stream.download()\n",
    "    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Function to save clips of batsmen\n",
    "def save_batsmen_clips(frames):\n",
    "    for i in range(len(frames)):\n",
    "        # We will consider the last 5 frames before hitting the ball\n",
    "        start_frame = max(0, i-5)\n",
    "        end_frame = i+1\n",
    "        \n",
    "        # We will save a 1-2 seconds clip of batsmen frames just before hitting the ball\n",
    "        clip_frames = frames[start_frame:end_frame]\n",
    "        if len(clip_frames) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Saving the clip as a video file\n",
    "        file_name = f\"clip_{i}.mp4\"\n",
    "        out = cv2.VideoWriter(file_name, cv2.VideoWriter_fourcc(*'mp4v'), 25, (clip_frames[0].shape[1], clip_frames[0].shape[0]))\n",
    "        for frame in clip_frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "\n",
    "# Example usage\n",
    "video_url = \"https://www.youtube.com/watch?v=Fj_WWSAJ2qw\"  # Replace with the URL of the video\n",
    "frames = extract_frames(video_url)\n",
    "save_batsmen_clips(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Input URL of the video\n",
    "url = \"INSERT_VIDEO_URL_HERE\"\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "# Define the minimum number of frames for each clip\n",
    "min_frames_per_clip = 10\n",
    "\n",
    "# Define the maximum number of clips to save\n",
    "max_clips_to_save = 15\n",
    "\n",
    "# Initialize variables\n",
    "frames = []\n",
    "frame_count = 0\n",
    "clip_count = 0\n",
    "clip_start_time = None\n",
    "\n",
    "# Loop through each frame of the video\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If there are no more frames, break the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect people in the frame using HOGDescriptor\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    rects, weights = hog.detectMultiScale(gray, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # If there are exactly 4 people in the frame, and we don't already have a clip in progress, start a new clip\n",
    "    if len(rects) == 4 and clip_start_time is None:\n",
    "        clip_start_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "\n",
    "    # If there are exactly 4 people in the frame, and we already have a clip in progress, add the current frame to the clip\n",
    "    elif len(rects) == 4 and clip_start_time is not None:\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    # If there are not exactly 4 people in the frame, and we have a clip in progress, end the clip and save it\n",
    "    elif len(rects) != 4 and clip_start_time is not None:\n",
    "        clip_end_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        clip_duration = clip_end_time - clip_start_time\n",
    "\n",
    "        # If the clip is long enough, save it\n",
    "        if frame_count >= min_frames_per_clip and clip_count < max_clips_to_save:\n",
    "            clip_name = \"clip_\" + str(clip_count) + \".mp4\"\n",
    "            out = cv2.VideoWriter(clip_name, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frames[0].shape[1], frames[0].shape[0]))\n",
    "            for f in frames:\n",
    "                out.write(f)\n",
    "            out.release()\n",
    "            clip_count += 1\n",
    "\n",
    "        clip_start_time = None\n",
    "\n",
    "# Release the video stream\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pytube\n",
    "# Fetch video from URL using OpenCV\n",
    "def fetch_video_from_url(url):\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    return cap\n",
    "\n",
    "# Extract time stamps from the video\n",
    "def extract_time_stamps(cap):\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    timestamps = []\n",
    "    for i in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Detect people in the frame\n",
    "        hog = cv2.HOGDescriptor()\n",
    "        hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "        (rects, weights) = hog.detectMultiScale(frame, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "        # If there are either 4 or 5 people in the frame, take 10 frames\n",
    "        if len(rects) == 4 or len(rects) == 5:\n",
    "            start_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            for j in range(10):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "            end_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            timestamps.append((start_time, end_time))\n",
    "        # If we have already found 15 clips, break out of the loop\n",
    "        if len(timestamps) >= 15:\n",
    "            break\n",
    "    return timestamps\n",
    "\n",
    "# Save clips starting from start times and end times\n",
    "def save_clips(cap, timestamps, output_dir):\n",
    "    for i, (start_time, end_time) in enumerate(timestamps):\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, start_time)\n",
    "        clip_frames = []\n",
    "        while cap.get(cv2.CAP_PROP_POS_MSEC) < end_time:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            clip_frames.append(frame)\n",
    "        if clip_frames:\n",
    "            clip = np.concatenate(clip_frames, axis=1)\n",
    "            output_path = os.path.join(output_dir, f\"clip_{i+1}.mp4\")\n",
    "            writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 25, (clip.shape[1], clip.shape[0]))\n",
    "            for frame in clip_frames:\n",
    "                writer.write(frame)\n",
    "            writer.release()\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.youtube.com/watch?v=AFEZzf9_EHk\"\n",
    "video = pytube.YouTube(url)\n",
    "stream = video.streams.get_by_itag(22)\n",
    "file = stream.download()    \n",
    "cap = cv2.VideoCapture(file)\n",
    "tic=time.time()\n",
    "timestamps = extract_time_stamps(cap)\n",
    "save_clips(cap, timestamps, \"output_dir\")\n",
    "cap.release()\n",
    "print(\"total time :\", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1.0],\n",
       " [1.04, 1.6],\n",
       " [1.8, 1.8800000000000003],\n",
       " [1.92, 2.0],\n",
       " [2.44, 3.2],\n",
       " [3.2800000000000002, 3.4],\n",
       " [3.48, 4.16],\n",
       " [4.2, 4.4],\n",
       " [4.44, 4.520000000000001],\n",
       " [4.5600000000000005, 4.68],\n",
       " [4.72, 4.8],\n",
       " [4.96, 5.04],\n",
       " [5.12, 5.32],\n",
       " [5.36, 5.44],\n",
       " [6.8, 6.88]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_clips(video_path, clip_dir):\n",
    "    # Load video and get basic info\n",
    "    video = pytube.YouTube(url)\n",
    "    stream = video.streams.get_by_itag(22)\n",
    "    file = stream.download()    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    \n",
    "    # Define parameters for detecting persons\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # Define minimum and maximum clip durations in seconds\n",
    "    min_clip_duration = 1\n",
    "    max_clip_duration = 2\n",
    "    \n",
    "    # Initialize variables\n",
    "    timestamps = []\n",
    "    clip_count = 0\n",
    "    current_time = 0\n",
    "    \n",
    "    while current_time < duration and clip_count < 15:\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect persons in frame\n",
    "        rects, weights = hog.detectMultiScale(frame)\n",
    "        \n",
    "        # Check if there are at least 4 persons in frame\n",
    "        if len(rects) >= 4:\n",
    "            # Get start time of clip\n",
    "            start_time = current_time\n",
    "            \n",
    "            # Read frames until clip duration is between min_clip_duration and max_clip_duration\n",
    "            while current_time - start_time < min_clip_duration or current_time - start_time > max_clip_duration:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                current_time = cap.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "                \n",
    "                # Detect persons in frame\n",
    "                rects, weights = hog.detectMultiScale(frame)\n",
    "                \n",
    "                # Check if there are less than 4 persons in frame\n",
    "                if len(rects) < 4:\n",
    "                    break\n",
    "            \n",
    "            # Get end time of clip\n",
    "            end_time = current_time\n",
    "            \n",
    "            # Save clip\n",
    "            clip_count += 1\n",
    "            clip_path = os.path.join(clip_dir, f\"clip_{clip_count}.avi\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "            clip_writer = cv2.VideoWriter(clip_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, start_time*1000)\n",
    "            while cap.get(cv2.CAP_PROP_POS_MSEC)/1000 <= end_time:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                clip_writer.write(frame)\n",
    "            clip_writer.release()\n",
    "            \n",
    "            # Append timestamps to list\n",
    "            timestamps.append([start_time, end_time])\n",
    "        \n",
    "        current_time = cap.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return timestamps\n",
    "save_clips(url,'output_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fc661a6ad2e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pytube\n",
    "tic=time.time()\n",
    "\n",
    "# Given video url\n",
    "url = \"https://www.youtube.com/watch?v=AFEZzf9_EHk\"\n",
    "\n",
    "# Initialize video capture object\n",
    "video = pytube.YouTube(url)\n",
    "stream = video.streams.get_by_itag(22)\n",
    "file = stream.download()    \n",
    "cap = cv2.VideoCapture(file)\n",
    "\n",
    "# Define Haar cascade for detecting faces\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Initialize variables\n",
    "frame_count = 0\n",
    "timestamps = []\n",
    "start_time = None\n",
    "is_bowling = False\n",
    "\n",
    "# Loop through frames of the video\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # Read a frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If frame is not read, break\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Increment frame count\n",
    "    frame_count += 1\n",
    "    \n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces using Haar cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    # If there are exactly 2 faces in the frame\n",
    "    if len(faces) == 2:\n",
    "        \n",
    "        # Get coordinates of the two faces\n",
    "        (x1, y1, w1, h1) = faces[0]\n",
    "        (x2, y2, w2, h2) = faces[1]\n",
    "        \n",
    "        # If one of the faces is on the left and the other on the right\n",
    "        if x1 < x2:\n",
    "            left_face = faces[0]\n",
    "            right_face = faces[1]\n",
    "        else:\n",
    "            left_face = faces[1]\n",
    "            right_face = faces[0]\n",
    "        \n",
    "        # Calculate the distance between the two faces\n",
    "        distance = right_face[0] - (left_face[0] + left_face[2])\n",
    "        \n",
    "        # If the distance is greater than a threshold, bowler is bowling\n",
    "        if distance > 200:\n",
    "            if not is_bowling:\n",
    "                start_time = frame_count / 25\n",
    "                is_bowling = True\n",
    "        else:\n",
    "            if is_bowling:\n",
    "                end_time = frame_count / 25\n",
    "                timestamps.append([start_time, end_time])\n",
    "                is_bowling = False\n",
    "\n",
    "# Release video capture object\n",
    "cap.release()\n",
    "\n",
    "# Initialize variables\n",
    "clip_count = 0\n",
    "clip_length = 0\n",
    "clip_start_time = None\n",
    "out = None\n",
    "\n",
    "# Loop through timestamps and extract clips\n",
    "for timestamp in timestamps:\n",
    "    \n",
    "    # Calculate clip start and end times\n",
    "    start_time = timestamp[0] - 0.5\n",
    "    end_time = timestamp[1] + 0.5\n",
    "    \n",
    "    # Initialize video capture object\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    \n",
    "    # Loop through frames of the video\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read a frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If frame is not read, break\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "        \n",
    "        # If current frame is within the clip\n",
    "        if frame_count / 25 >= start_time and frame_count / 25 <= end_time:\n",
    "            \n",
    "            # If clip has not started yet\n",
    "            if clip_start_time is None:\n",
    "                clip_start_time = frame_count / 25\n",
    "            \n",
    "            # Add frame to clip\n",
    "            if out is None:\n",
    "                clip_length += 1\n",
    "                out = cv2.VideoWriter(f\"clip_{clip_count}.avi\", cv2.VideoWriter_fourcc(*'MJPG'), 25, (frame.shape[1], frame.shape[0]))\n",
    "            out.write(frame)\n",
    "        elif clip_start_time is not None:        \n",
    "            # Release video writer object\n",
    "            out.release()\n",
    "            out = None\n",
    "\n",
    "            # Print clip information\n",
    "            print(f\"Clip {clip_count}: Start time = {clip_start_time}, End time = {frame_count / 25}, Length = {clip_length}\")\n",
    "\n",
    "            # Reset variables\n",
    "            clip_count += 1\n",
    "            clip_start_time = None\n",
    "            clip_length = 0\n",
    "\n",
    "# Release video capture object\n",
    "cap.release()\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times=[]\n",
    "end_times=[]\n",
    "for i in timestamps:\n",
    "    start_times.append(i[0])\n",
    "    end_times.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
